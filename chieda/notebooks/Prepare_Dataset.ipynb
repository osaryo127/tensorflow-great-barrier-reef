{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e51af-4d82-4516-a2d3-a14ab65534d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "28bbc49a-a8d5-42b2-9aa6-52f188f1198d",
   "metadata": {
    "papermill": {
     "duration": 0.042519,
     "end_time": "2021-11-27T11:42:20.763480",
     "exception": false,
     "start_time": "2021-11-27T11:42:20.720961",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# [Tensorflow - Help Protect the Great Barrier Reef](https://www.kaggle.com/c/tensorflow-great-barrier-reef)\n",
    "> 水中の画像データからオニヒトデを検出する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29984709-2d93-4353-9acf-8722444845fe",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 表示用設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8edcc1cb-5a33-4dfd-aa52-eed17b77e42c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  table {margin-left: 0 !important;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {margin-left: 0 !important;}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f101cef1-35eb-4235-ab81-6d55ce62e403",
   "metadata": {
    "papermill": {
     "duration": 0.040299,
     "end_time": "2021-11-27T11:42:20.846452",
     "exception": false,
     "start_time": "2021-11-27T11:42:20.806153",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 📒 Notebooks:\n",
    "copy&edit\n",
    "* Train: [Great-Barrier-Reef: YOLOv5 [train] 🌊](https://www.kaggle.com/awsaf49/great-barrier-reef-yolov5-train)\n",
    "* Infer: [Great-Barrier-Reef: YOLOv5 [infer] 🌊](https://www.kaggle.com/awsaf49/great-barrier-reef-yolov5-infer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9403a364-a997-47ec-a4f2-5d42a12045a4",
   "metadata": {
    "papermill": {
     "duration": 0.039386,
     "end_time": "2021-11-27T11:42:20.926160",
     "exception": false,
     "start_time": "2021-11-27T11:42:20.886774",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 🛠 必要なライブラリのインストール\n",
    "* imagesize: 画像のサイズを測定できる\n",
    "* wandb: 実験管理ができるライブラリ\n",
    "    * [参考サイト](https://www.nogawanogawa.com/entry/wandb)\n",
    "    * [公式チュートリアル](https://docs.wandb.ai/)\n",
    "* libstdc++6: おそらくopencvのために必要なもの"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78f8ba87-53ba-4383-9714-ebe0f8dd389e",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "papermill": {
     "duration": 262.258908,
     "end_time": "2021-11-27T11:46:43.224919",
     "exception": false,
     "start_time": "2021-11-27T11:42:20.966011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install -q imagesize\n",
    "# !pip install -qU wandb\n",
    "# !add-apt-repository ppa:ubuntu-toolchain-r/test -y\n",
    "# !apt-get update\n",
    "# !apt-get upgrade libstdc++6 -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304dafda-6500-4af5-afe6-3038c2c4ce9a",
   "metadata": {
    "papermill": {
     "duration": 0.111553,
     "end_time": "2021-11-27T11:46:43.448178",
     "exception": false,
     "start_time": "2021-11-27T11:46:43.336625",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 📚 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecbf195c-240a-4841-bdb8-17014d2671c6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.543054,
     "end_time": "2021-11-27T11:46:44.170009",
     "exception": false,
     "start_time": "2021-11-27T11:46:43.626955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import cv2\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt\n",
    "# import cupy as cp\n",
    "import ast\n",
    "import glob\n",
    "from sklearn import model_selection\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "import shutil\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "# sys.path.append('../input/tensorflow-great-barrier-reef')\n",
    "os_kind = sys.platform\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "import imagesize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2704e769-6402-4798-8f5b-2e8dda2254fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 📌 キーポイント\n",
    "* これまでの物体検出コンペと違って**python time-series API**を使って予測が行われる\n",
    "* BBoxの提出フォーマットはCOCO形式 `[x_min, y_min, width, height]`\n",
    "* 今回の評価指標は`F2`なので、Recall重視の方が評価指標が上がりやすい。つまり、オニヒトデの誤検出より、見逃し減らすようにした方がスコアが上がりやすい\n",
    "$$F2 = 5 \\cdot \\frac{precision \\cdot recall}{4\\cdot precision + recall}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc0f7d3c-926f-4502-be4e-175a9afcae2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "papermill": {
     "duration": 0.107722,
     "end_time": "2021-11-27T11:46:44.942929",
     "exception": false,
     "start_time": "2021-11-27T11:46:44.835207",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ⭐ WandB\n",
    "\n",
    "Weights & Biases (W&B) は、MLOpsの実験をトラッキングするためのプラットフォーム。実験のトラッキング、データセットのバージョニング、モデルの管理ができる。\n",
    "\n",
    "W&Bの良い機能を一部紹介(copy元の著者由来)\n",
    "\n",
    "* 実験の追跡、比較、可視化\n",
    "* ライブメトリクス(live metrics)、ターミナルログ、システム統計情報(system stats)をダッシュボードで見れる\n",
    "* モデルのバージョンがどのように改善されたかをグラフで示せる"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e689c11-157d-431a-8387-875bfde1c85a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ⚙実行前設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb186ef-520e-482d-8eec-132e5ed052ea",
   "metadata": {
    "papermill": {
     "duration": 0.105829,
     "end_time": "2021-11-27T11:46:45.155045",
     "exception": false,
     "start_time": "2021-11-27T11:46:45.049216",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 📖 Meta Data\n",
    "* `train_images/` - `video_{video_id}/{video_frame}.jpg`という形での学習画像が含まれるフォルダ\n",
    "\n",
    "* `[train/test].csv` - 画像のメタ情報。テストのメタデータはsubmitしたときにダウンロードされる。submit前は3つの画像に関する情報しかない\n",
    "* `video_id` - 画像の含まれるvideoのID番号。video_idの順序に特に意味はない。\n",
    "* `video_frame` - video中に含まれる画像のフレーム番号。同じvideo_idでもダイバーの潜水、浮上があったりするため、急に視点が変わったりする。\n",
    "* `sequence` - 先ほどのダイバーの潜水、浮上ごとにvideoを細かく分けた時のid。順序に大きな意味はない(著者はこう言ってるが...本当に意味がないのか?)\n",
    "* `sequence_frame` - sequence中の画像のフレーム番号。\n",
    "* `image_id` - `{video_id}-{video_frame}`\n",
    "* `annotations` - オニヒトデのbbox情報。提出する際のフォーマットではないことに注意。`test.csv`では利用できない。このbbox情報の形式はbboxの左下の座標`(x_min, y_min)`と、そこからの`width`と`height`で記載されていて、これはCOCO formatと呼ばれるらしい。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba59f703-3ca6-4f59-9439-246723d98111",
   "metadata": {
    "papermill": {
     "duration": 0.113584,
     "end_time": "2021-11-27T11:46:45.373743",
     "exception": false,
     "start_time": "2021-11-27T11:46:45.260159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# FOLD = 4 # which fold to train 0 ~ 4\n",
    "REMOVE_NOBBOX = True # remove images with no bbox\n",
    "# DATA_DIR  = '/workspace/cots/data'\n",
    "DATA_DIR  = '../data'\n",
    "IMAGE_DIR = f'{DATA_DIR}/images' # directory to save images\n",
    "LABEL_DIR = f'{DATA_DIR}/labels' # directory to save labels\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84719ef-5952-4fc0-8228-b3ff3d972e66",
   "metadata": {
    "papermill": {
     "duration": 0.105998,
     "end_time": "2021-11-27T11:46:45.585298",
     "exception": false,
     "start_time": "2021-11-27T11:46:45.479300",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96b289fd-0373-4549-9ad8-4d35a2787f46",
   "metadata": {
    "papermill": {
     "duration": 1.409109,
     "end_time": "2021-11-27T11:46:47.100536",
     "exception": false,
     "start_time": "2021-11-27T11:46:45.691427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -p : 必要に応じて親ディレクトリも作成\n",
    "if os_kind == 'win32':\n",
    "    os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "    os.makedirs(LABEL_DIR, exist_ok=True)\n",
    "else:\n",
    "    !mkdir -p {IMAGE_DIR}\n",
    "    !mkdir -p {LABEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe962710-d42d-4c02-8f20-06f4b5b531c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 💼データの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc78b6fb-2980-4c12-bba4-78094a2ccf5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 🔧Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06c34aa7-e7cc-4c4e-8d5f-06cad048277b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_fold(df: pd.DataFrame, n_folds: int = 5, seed: int = 42, show_fold_info: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"train.csvに交差検証用のFOLDを付与する\"\"\"\n",
    "    # アノテーション数\n",
    "    df[\"annotations\"] = df[\"annotations\"].apply(ast.literal_eval)\n",
    "    df[\"n_annotations\"] = df[\"annotations\"].str.len()\n",
    "    df[\"has_annotations\"] = df[\"annotations\"].str.len() > 0\n",
    "    df[\"doesnt_have_annotations\"] = df[\"annotations\"].str.len() == 0\n",
    "\n",
    "    # 物体の有無によるシーケンスの分割\n",
    "    df[\"start_cut_here\"] = (\n",
    "        df[\"has_annotations\"] & df[\"doesnt_have_annotations\"].shift(1) & df[\"doesnt_have_annotations\"].shift(2)\n",
    "    )\n",
    "    df[\"end_cut_here\"] = df[\"doesnt_have_annotations\"] & df[\"has_annotations\"].shift(1) & df[\"has_annotations\"].shift(2)\n",
    "    df[\"sequence_change\"] = df[\"sequence\"] != df[\"sequence\"].shift(1)\n",
    "    df[\"last_row\"] = df.index == len(df) - 1\n",
    "    df[\"cut_here\"] = df[\"start_cut_here\"] | df[\"end_cut_here\"] | df[\"sequence_change\"] | df[\"last_row\"]\n",
    "    start_idx = 0\n",
    "    for subsequence_id, end_idx in enumerate(df[df[\"cut_here\"]].index):\n",
    "        df.loc[start_idx:end_idx, \"subsequence_id\"] = subsequence_id\n",
    "        start_idx = end_idx\n",
    "    df[\"subsequence_id\"] = df[\"subsequence_id\"].astype(int)\n",
    "    drop_cols = [\"start_cut_here\", \"end_cut_here\", \"sequence_change\", \"last_row\", \"cut_here\", \"doesnt_have_annotations\"]\n",
    "    df = df.drop(drop_cols, axis=1)\n",
    "\n",
    "    # 物体の有無による階層化を用いたsubsequenceのfold振り分け\n",
    "    df_split = (\n",
    "        df.groupby(\"subsequence_id\").agg({\"has_annotations\": \"max\", \"video_frame\": \"count\"}).astype(int).reset_index()\n",
    "    )\n",
    "    kf = model_selection.StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n",
    "    for fold_id, (_, val_idx) in enumerate(kf.split(df_split[\"subsequence_id\"], y=df_split[\"has_annotations\"])):\n",
    "        subseq_val_idx = df_split[\"subsequence_id\"].iloc[val_idx]\n",
    "        df.loc[df[\"subsequence_id\"].isin(subseq_val_idx), \"fold\"] = fold_id\n",
    "        if show_fold_info:\n",
    "            print(f\"fold {fold_id} : {subseq_val_idx.values}\")\n",
    "\n",
    "    df[\"fold\"] = df[\"fold\"].astype(int)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fadaddb-4487-4ab1-ac12-e2ce1132a6dc",
   "metadata": {
    "papermill": {
     "duration": 0.114065,
     "end_time": "2021-11-27T11:46:47.541052",
     "exception": false,
     "start_time": "2021-11-27T11:46:47.426987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_path(row):\n",
    "    \"\"\"\n",
    "    画像のpathを格納する\n",
    "    \"\"\"\n",
    "    row['old_image_path'] = f'{DATA_DIR}/train_images/video_{row.video_id}/{row.video_frame}.jpg'\n",
    "    row['image_path'] = f'{IMAGE_DIR}/video_{row.video_id}_{row.video_frame}.jpg'\n",
    "    row['label_path'] = f'{LABEL_DIR}/video_{row.video_id}_{row.video_frame}.txt'\n",
    "    return row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4052bdb0-48ef-49ef-b033-03fee6ccd82e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## train.csv読み込みと補助カラム付加\n",
    "付加する補助カラム一覧\n",
    "\n",
    "| Column Name    | Description                  |\n",
    "| -------------- | ---------------------------- |\n",
    "| old_image_path | オリジナル画像パス           |\n",
    "| image_path     | 加工後の画像パス             |\n",
    "| label_path     | ラベルパス                   |\n",
    "| fold           | 検証データになるフォールドID |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93d47a97-f973-4837-9cf1-c129d3397757",
   "metadata": {
    "papermill": {
     "duration": 40.245851,
     "end_time": "2021-11-27T11:47:27.893054",
     "exception": false,
     "start_time": "2021-11-27T11:46:47.647203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>video_frame</th>\n",
       "      <th>sequence_frame</th>\n",
       "      <th>image_id</th>\n",
       "      <th>annotations</th>\n",
       "      <th>old_image_path</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>n_annotations</th>\n",
       "      <th>has_annotations</th>\n",
       "      <th>subsequence_id</th>\n",
       "      <th>fold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0-0</td>\n",
       "      <td>[]</td>\n",
       "      <td>../data/train_images/video_0/0.jpg</td>\n",
       "      <td>../data/images/video_0_0.jpg</td>\n",
       "      <td>../data/labels/video_0_0.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0-1</td>\n",
       "      <td>[]</td>\n",
       "      <td>../data/train_images/video_0/1.jpg</td>\n",
       "      <td>../data/images/video_0_1.jpg</td>\n",
       "      <td>../data/labels/video_0_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   video_id  sequence  video_frame  sequence_frame image_id annotations  \\\n",
       "0         0     40258            0               0      0-0          []   \n",
       "1         0     40258            1               1      0-1          []   \n",
       "\n",
       "                       old_image_path                    image_path  \\\n",
       "0  ../data/train_images/video_0/0.jpg  ../data/images/video_0_0.jpg   \n",
       "1  ../data/train_images/video_0/1.jpg  ../data/images/video_0_1.jpg   \n",
       "\n",
       "                     label_path  n_annotations  has_annotations  \\\n",
       "0  ../data/labels/video_0_0.txt              0            False   \n",
       "1  ../data/labels/video_0_1.txt              0            False   \n",
       "\n",
       "   subsequence_id  fold  \n",
       "0               1     0  \n",
       "1               1     0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train Data\n",
    "df = pd.read_csv(f'{DATA_DIR}/train.csv')\n",
    "df = df.apply(get_path, axis=1)\n",
    "df = add_fold(df, n_folds=5, seed=SEED)\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e81a68-b51b-46ca-9944-dce8f1d7c4ff",
   "metadata": {
    "papermill": {
     "duration": 0.111017,
     "end_time": "2021-11-27T11:47:28.648489",
     "exception": false,
     "start_time": "2021-11-27T11:47:28.537472",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Clean Data (オニヒトデが写ってるデータのみに)\n",
    "\n",
    "- 学習データはオニヒトデが写ってるデータのみに制限する(copy元の著者は簡単に実験できるからこうしているとのこと)\n",
    "- オニヒトデが写っていないデータを学習にうまいこと使う　or　アノテーション漏れしてるデータを追加するのは有効かも\n",
    "- trainの大体80%はオニヒトデが写ってない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa955c26-aa35-4373-9dbc-2f4326071a77",
   "metadata": {
    "papermill": {
     "duration": 0.209493,
     "end_time": "2021-11-27T11:47:28.428087",
     "exception": false,
     "start_time": "2021-11-27T11:47:28.218594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # df['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\n",
    "# df['num_bbox'] = df['annotations'].apply(lambda x: len(x))\n",
    "# data = (df.num_bbox>0).value_counts(normalize=True)*100\n",
    "# print(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")\n",
    "# if REMOVE_NOBBOX:\n",
    "#     df = df.query(\"num_bbox > 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a65cdd-3e9f-4e69-a1a6-161cd065ed3b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 🎈Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0ac3d7-a565-4374-b7b8-558c38a4659d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9499f650-f6fd-460f-934f-f568a628e4a5",
   "metadata": {
    "papermill": {
     "duration": 0.113327,
     "end_time": "2021-11-27T11:47:29.123571",
     "exception": false,
     "start_time": "2021-11-27T11:47:29.010244",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ✏️ Write Images (学習するための画像を抽出)\n",
    "\n",
    "* kaggle　notebook上で行うときには、kaggle notebookのデフォルトのcurrent directory(`/kaggle/working`)に(`/kaggle/input`)からコピーしないと、書き込み権限が`/kaggle/input`にないため、yolov5を動かした時にエラーになる\n",
    "* この処理は**Joblib**の**Parallel**で高速化できる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25be43bf-a270-4b91-aba6-70b576531468",
   "metadata": {
    "papermill": {
     "duration": 0.118096,
     "end_time": "2021-11-27T11:47:29.353381",
     "exception": false,
     "start_time": "2021-11-27T11:47:29.235285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_copy(path):\n",
    "    data = path.split('/')\n",
    "    filename = data[-1]\n",
    "    video_id = data[-2]\n",
    "    new_path = os.path.join(IMAGE_DIR,f'{video_id}_{filename}')\n",
    "    shutil.copy(path, new_path)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c14d9fd-0de6-4111-9cbe-d778fcafea0f",
   "metadata": {
    "papermill": {
     "duration": 28.524822,
     "end_time": "2021-11-27T11:47:57.990053",
     "exception": false,
     "start_time": "2021-11-27T11:47:29.465231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd31535fe25d497abb77fd474a9c5e3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from os.path import exists\n",
    "path = image_paths[0]\n",
    "\n",
    "# すでにある場合はスキップ\n",
    "if not exists(os.path.join(IMAGE_DIR,f'{path.split('/')[-2]}_{path.split('/')[-1]}')):\n",
    "    image_paths = df.old_image_path.tolist()\n",
    "    _ = Parallel(n_jobs=-1, backend='threading')(delayed(make_copy)(path) for path in tqdm(image_paths))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9eedd7-181c-4026-afe7-fdd5ed7c6a8b",
   "metadata": {
    "papermill": {
     "duration": 0.112018,
     "end_time": "2021-11-27T11:47:58.221801",
     "exception": false,
     "start_time": "2021-11-27T11:47:58.109783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 🔨 Model Helper\n",
    "\n",
    "主に\n",
    "* bboxのアノテーションの形式(coco->yolo, voc->yolo など)を変える関数\n",
    "* 画像を描画するための関数\n",
    "が定義されてる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43412d83-73dd-4c8e-ac6e-1a9539981a14",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 0.170947,
     "end_time": "2021-11-27T11:47:58.520553",
     "exception": false,
     "start_time": "2021-11-27T11:47:58.349606",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def voc2yolo(image_height, image_width, bboxes):\n",
    "    \"\"\"\n",
    "    voc  => [x1, y1, x2, y1]\n",
    "    yolo => [xmid, ymid, w, h] (normalized)\n",
    "    \"\"\"\n",
    "\n",
    "    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n",
    "    \n",
    "    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]/ image_width\n",
    "    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]/ image_height\n",
    "    \n",
    "    w = bboxes[..., 2] - bboxes[..., 0]\n",
    "    h = bboxes[..., 3] - bboxes[..., 1]\n",
    "    \n",
    "    bboxes[..., 0] = bboxes[..., 0] + w/2\n",
    "    bboxes[..., 1] = bboxes[..., 1] + h/2\n",
    "    bboxes[..., 2] = w\n",
    "    bboxes[..., 3] = h\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "def yolo2voc(image_height, image_width, bboxes):\n",
    "    \"\"\"\n",
    "    yolo => [xmid, ymid, w, h] (normalized)\n",
    "    voc  => [x1, y1, x2, y1]\n",
    "    \n",
    "    \"\"\" \n",
    "    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n",
    "    \n",
    "    bboxes[..., [0, 2]] = bboxes[..., [0, 2]]* image_width\n",
    "    bboxes[..., [1, 3]] = bboxes[..., [1, 3]]* image_height\n",
    "    \n",
    "    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n",
    "    bboxes[..., [2, 3]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "def coco2yolo(image_height, image_width, bboxes):\n",
    "    \"\"\"\n",
    "    cocoフォーマットから、yoloフォーマット\n",
    "    coco => [xmin, ymin, w, h]\n",
    "    yolo => [xmid, ymid, w, h] (normalized)\n",
    "    \"\"\"\n",
    "    \n",
    "    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n",
    "    \n",
    "    # normolizinig\n",
    "    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]/ image_width\n",
    "    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]/ image_height\n",
    "    \n",
    "    # converstion (xmin, ymin) => (xmid, ymid)\n",
    "    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] + bboxes[..., [2, 3]]/2\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "def yolo2coco(image_height, image_width, bboxes):\n",
    "    \"\"\"\n",
    "    yoloフォーマットからcocoフォーマット\n",
    "    yolo => [xmid, ymid, w, h] (normalized)\n",
    "    coco => [xmin, ymin, w, h]\n",
    "    \n",
    "    \"\"\" \n",
    "    bboxes = bboxes.copy().astype(float) # otherwise all value will be 0 as voc_pascal dtype is np.int\n",
    "    \n",
    "    # denormalizing\n",
    "    bboxes[..., [0, 2]]= bboxes[..., [0, 2]]* image_width\n",
    "    bboxes[..., [1, 3]]= bboxes[..., [1, 3]]* image_height\n",
    "    \n",
    "    # converstion (xmid, ymid) => (xmin, ymin) \n",
    "    bboxes[..., [0, 1]] = bboxes[..., [0, 1]] - bboxes[..., [2, 3]]/2\n",
    "    \n",
    "    return bboxes\n",
    "\n",
    "\n",
    "def voc2coco(bboxes, image_height=720, image_width=1280):\n",
    "    bboxes  = voc2yolo(image_height, image_width, bboxes)\n",
    "    bboxes  = yolo2coco(image_height, image_width, bboxes)\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "def load_image(image_path):\n",
    "    return cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "def plot_one_box(x, img, color=None, label=None, line_thickness=None):\n",
    "    # Plots one bounding box on image img\n",
    "    tl = line_thickness or round(0.002 * (img.shape[0] + img.shape[1]) / 2) + 1  # line/font thickness\n",
    "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
    "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
    "    cv2.rectangle(img, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
    "    if label:\n",
    "        tf = max(tl - 1, 1)  # font thickness\n",
    "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
    "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
    "        cv2.rectangle(img, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
    "        cv2.putText(img, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
    "\n",
    "def draw_bboxes(img, bboxes, classes, class_ids, colors = None, show_classes = None, bbox_format = 'yolo', class_name = False, line_thickness = 2):  \n",
    "     \n",
    "    image = img.copy()\n",
    "    show_classes = classes if show_classes is None else show_classes\n",
    "    colors = (0, 255 ,0) if colors is None else colors\n",
    "    \n",
    "    if bbox_format == 'yolo':\n",
    "        \n",
    "        for idx in range(len(bboxes)):  \n",
    "            \n",
    "            bbox  = bboxes[idx]\n",
    "            cls   = classes[idx]\n",
    "            cls_id = class_ids[idx]\n",
    "            color = colors[cls_id] if type(colors) is list else colors\n",
    "            \n",
    "            if cls in show_classes:\n",
    "            \n",
    "                x1 = round(float(bbox[0])*image.shape[1])\n",
    "                y1 = round(float(bbox[1])*image.shape[0])\n",
    "                w  = round(float(bbox[2])*image.shape[1]/2) #w/2 \n",
    "                h  = round(float(bbox[3])*image.shape[0]/2)\n",
    "\n",
    "                voc_bbox = (x1-w, y1-h, x1+w, y1+h)\n",
    "                plot_one_box(voc_bbox, \n",
    "                             image,\n",
    "                             color = color,\n",
    "                             label = cls if class_name else str(get_label(cls)),\n",
    "                             line_thickness = line_thickness)\n",
    "            \n",
    "    elif bbox_format == 'coco':\n",
    "        \n",
    "        for idx in range(len(bboxes)):  \n",
    "            \n",
    "            bbox  = bboxes[idx]\n",
    "            cls   = classes[idx]\n",
    "            cls_id = class_ids[idx]\n",
    "            color = colors[cls_id] if type(colors) is list else colors\n",
    "            \n",
    "            if cls in show_classes:            \n",
    "                x1 = int(round(bbox[0]))\n",
    "                y1 = int(round(bbox[1]))\n",
    "                w  = int(round(bbox[2]))\n",
    "                h  = int(round(bbox[3]))\n",
    "\n",
    "                voc_bbox = (x1, y1, x1+w, y1+h)\n",
    "                plot_one_box(voc_bbox, \n",
    "                             image,\n",
    "                             color = color,\n",
    "                             label = cls if class_name else str(cls_id),\n",
    "                             line_thickness = line_thickness)\n",
    "\n",
    "    elif bbox_format == 'voc_pascal':\n",
    "        \n",
    "        for idx in range(len(bboxes)):  \n",
    "            \n",
    "            bbox  = bboxes[idx]\n",
    "            cls   = classes[idx]\n",
    "            cls_id = class_ids[idx]\n",
    "            color = colors[cls_id] if type(colors) is list else colors\n",
    "            \n",
    "            if cls in show_classes: \n",
    "                x1 = int(round(bbox[0]))\n",
    "                y1 = int(round(bbox[1]))\n",
    "                x2 = int(round(bbox[2]))\n",
    "                y2 = int(round(bbox[3]))\n",
    "                voc_bbox = (x1, y1, x2, y2)\n",
    "                plot_one_box(voc_bbox, \n",
    "                             image,\n",
    "                             color = color,\n",
    "                             label = cls if class_name else str(cls_id),\n",
    "                             line_thickness = line_thickness)\n",
    "    else:\n",
    "        raise ValueError('wrong bbox format')\n",
    "\n",
    "    return image\n",
    "\n",
    "def get_bbox(annots):\n",
    "    bboxes = [list(annot.values()) for annot in annots]\n",
    "    return bboxes\n",
    "\n",
    "def get_imgsize(row):\n",
    "    \"\"\"\n",
    "    画像自体のサイズの獲得\n",
    "    \"\"\"\n",
    "    row['width'], row['height'] = imagesize.get(row['image_path'])\n",
    "    return row\n",
    "\n",
    "\n",
    "def show_img(img, bboxes, bbox_format='yolo'):\n",
    "    names  = ['starfish']*len(bboxes)\n",
    "    labels = [0]*len(bboxes)\n",
    "    img    = draw_bboxes(img = img,\n",
    "                           bboxes = bboxes, \n",
    "                           classes = names,\n",
    "                           class_ids = labels,\n",
    "                           class_name = True, \n",
    "                           colors = colors, \n",
    "                           bbox_format = bbox_format,\n",
    "                           line_thickness = 2)\n",
    "    return Image.fromarray(img).resize((800, 400))\n",
    "\n",
    "\n",
    "np.random.seed(32)\n",
    "colors = [(np.random.randint(255), np.random.randint(255), np.random.randint(255))\\\n",
    "          for idx in range(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb66598-8e41-494f-bc1a-70a90fa02575",
   "metadata": {
    "papermill": {
     "duration": 0.113032,
     "end_time": "2021-11-27T11:47:58.751852",
     "exception": false,
     "start_time": "2021-11-27T11:47:58.638820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create BBox (数値だけの形に)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2687d031-0695-4944-bedd-67ae9c462b77",
   "metadata": {
    "papermill": {
     "duration": 0.183581,
     "end_time": "2021-11-27T11:47:59.045166",
     "exception": false,
     "start_time": "2021-11-27T11:47:58.861585",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>video_frame</th>\n",
       "      <th>sequence_frame</th>\n",
       "      <th>image_id</th>\n",
       "      <th>annotations</th>\n",
       "      <th>old_image_path</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>n_annotations</th>\n",
       "      <th>has_annotations</th>\n",
       "      <th>subsequence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>bboxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0-0</td>\n",
       "      <td>[]</td>\n",
       "      <td>../data/train_images/video_0/0.jpg</td>\n",
       "      <td>../data/images/video_0_0.jpg</td>\n",
       "      <td>../data/labels/video_0_0.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0-1</td>\n",
       "      <td>[]</td>\n",
       "      <td>../data/train_images/video_0/1.jpg</td>\n",
       "      <td>../data/images/video_0_1.jpg</td>\n",
       "      <td>../data/labels/video_0_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   video_id  sequence  video_frame  sequence_frame image_id annotations  \\\n",
       "0         0     40258            0               0      0-0          []   \n",
       "1         0     40258            1               1      0-1          []   \n",
       "\n",
       "                       old_image_path                    image_path  \\\n",
       "0  ../data/train_images/video_0/0.jpg  ../data/images/video_0_0.jpg   \n",
       "1  ../data/train_images/video_0/1.jpg  ../data/images/video_0_1.jpg   \n",
       "\n",
       "                     label_path  n_annotations  has_annotations  \\\n",
       "0  ../data/labels/video_0_0.txt              0            False   \n",
       "1  ../data/labels/video_0_1.txt              0            False   \n",
       "\n",
       "   subsequence_id  fold bboxes  \n",
       "0               1     0     []  \n",
       "1               1     0     []  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df['bboxes'] = df.annotations.progress_apply(get_bbox)\n",
    "df['bboxes'] = df.annotations.apply(get_bbox)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80935cde-08d9-491e-a8d3-fbffb2a298a4",
   "metadata": {
    "papermill": {
     "duration": 0.111231,
     "end_time": "2021-11-27T11:47:59.268648",
     "exception": false,
     "start_time": "2021-11-27T11:47:59.157417",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Get Image-Size (画像(jpg)のサイズは均等かどうか)\n",
    "> 全部の画像のサイズが(横, 縦) = (1280, 720)になってる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca5b2563-e39b-4996-8d2a-e7d6027e498f",
   "metadata": {
    "papermill": {
     "duration": 6.648103,
     "end_time": "2021-11-27T11:48:06.027859",
     "exception": false,
     "start_time": "2021-11-27T11:47:59.379756",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1280], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([720], dtype=int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>video_frame</th>\n",
       "      <th>sequence_frame</th>\n",
       "      <th>image_id</th>\n",
       "      <th>annotations</th>\n",
       "      <th>old_image_path</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>n_annotations</th>\n",
       "      <th>has_annotations</th>\n",
       "      <th>subsequence_id</th>\n",
       "      <th>fold</th>\n",
       "      <th>bboxes</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0-0</td>\n",
       "      <td>[]</td>\n",
       "      <td>../data/train_images/video_0/0.jpg</td>\n",
       "      <td>../data/images/video_0_0.jpg</td>\n",
       "      <td>../data/labels/video_0_0.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>40258</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0-1</td>\n",
       "      <td>[]</td>\n",
       "      <td>../data/train_images/video_0/1.jpg</td>\n",
       "      <td>../data/images/video_0_1.jpg</td>\n",
       "      <td>../data/labels/video_0_1.txt</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>1280</td>\n",
       "      <td>720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   video_id  sequence  video_frame  sequence_frame image_id annotations  \\\n",
       "0         0     40258            0               0      0-0          []   \n",
       "1         0     40258            1               1      0-1          []   \n",
       "\n",
       "                       old_image_path                    image_path  \\\n",
       "0  ../data/train_images/video_0/0.jpg  ../data/images/video_0_0.jpg   \n",
       "1  ../data/train_images/video_0/1.jpg  ../data/images/video_0_1.jpg   \n",
       "\n",
       "                     label_path  n_annotations  has_annotations  \\\n",
       "0  ../data/labels/video_0_0.txt              0            False   \n",
       "1  ../data/labels/video_0_1.txt              0            False   \n",
       "\n",
       "   subsequence_id  fold bboxes  width  height  \n",
       "0               1     0     []   1280     720  \n",
       "1               1     0     []   1280     720  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df = df.apply(get_imgsize,axis=1)\n",
    "df['width'] = 1280\n",
    "df['height'] = 720\n",
    "display(df.width.unique(), df.height.unique())\n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b795d2-3e6f-4578-97cc-45225a41d89d",
   "metadata": {
    "papermill": {
     "duration": 0.116094,
     "end_time": "2021-11-27T11:48:06.269698",
     "exception": false,
     "start_time": "2021-11-27T11:48:06.153604",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 🏷️ Create Labels (YOLOのフォーマットのBBox作成)\n",
    "コンペのbboxフォーマットはCOCOのものなのでYOLOのformatに変える必要がある \n",
    "\n",
    "yolov5の入力のために各画像ごとのbboxの情報を記載する`*.txt`が必要。`*.txt`の細かい指定は下記参照。\n",
    "\n",
    "* オブジェクトごとに1行 (1枚の画像にオニヒトデが2つあるなら２行になる)\n",
    "* 各行は`class_no, x_center, y_center, width, height`という形式 (bboxはyolo形式ということ)\n",
    "* bboxは0~1で正規化されていないといけない。つまり、`x_center`と`width`は画像の横(`image_width`)で、`y_center`と`height`は画像の縦(`image_height`)で除算する。\n",
    "* クラスは0始まり (そもそも今回はオニヒトデだけなので0だけだが)\n",
    "\n",
    "> コンペのbboxの形式はCOCO形式(`[x_min, y_min, width, height]`)なので、yolo形式(`[x_center, y_center, width, height]`)に変換する必要がある"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f552b9b-7be3-435e-a186-8e9ecc66b52b",
   "metadata": {
    "_kg_hide-input": true,
    "papermill": {
     "duration": 2.777694,
     "end_time": "2021-11-27T11:48:09.160487",
     "exception": false,
     "start_time": "2021-11-27T11:48:06.382793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241b6e795fb047cc8783b664e4bb8bef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing: 18582\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "all_bboxes = []\n",
    "for row_idx in tqdm(range(df.shape[0])):\n",
    "    row = df.iloc[row_idx]\n",
    "    image_height = row.height\n",
    "    image_width  = row.width\n",
    "    bboxes_coco  = np.array(row.bboxes).astype(np.float32).copy()\n",
    "    num_bbox     = len(bboxes_coco)\n",
    "    names        = ['cots']*num_bbox\n",
    "    labels       = [0]*num_bbox\n",
    "    ## Create Annotation(YOLO)\n",
    "    with open(row.label_path, 'w') as f:\n",
    "        if num_bbox<1:\n",
    "            annot = ''\n",
    "            f.write(annot)\n",
    "            cnt+=1\n",
    "            continue\n",
    "        bboxes_yolo  = coco2yolo(image_height, image_width, bboxes_coco)\n",
    "        bboxes_yolo  = np.clip(bboxes_yolo, 0, 1)\n",
    "        all_bboxes.extend(bboxes_yolo)\n",
    "        for bbox_idx in range(len(bboxes_yolo)):\n",
    "            annot = [str(labels[bbox_idx])]+ list(bboxes_yolo[bbox_idx].astype(str))+(['\\n'] if num_bbox!=(bbox_idx+1) else [''])\n",
    "            annot = ' '.join(annot)\n",
    "            annot = annot.strip(' ')\n",
    "            f.write(annot)\n",
    "print('Missing:',cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d9c4c0-c978-4803-8793-f6c08b501602",
   "metadata": {
    "papermill": {
     "duration": 0.304484,
     "end_time": "2021-11-27T11:48:25.503724",
     "exception": false,
     "start_time": "2021-11-27T11:48:25.199240",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 🍚 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77554429-ca42-4745-8cc8-4ef9130ed270",
   "metadata": {
    "papermill": {
     "duration": 0.325276,
     "end_time": "2021-11-27T11:48:26.136342",
     "exception": false,
     "start_time": "2021-11-27T11:48:25.811066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19509, 3992)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files = []\n",
    "val_files   = []\n",
    "fold = 0\n",
    "train_df = df.query(f\"fold!={fold}\")\n",
    "valid_df = df.query(f\"fold=={fold}\")\n",
    "train_files += list(train_df.image_path.unique())\n",
    "val_files += list(valid_df.image_path.unique())\n",
    "len(train_files), len(val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "88fd834e-e0c3-44a1-ac83-ece000789de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:  83.0 %\n",
      "valid:  17.0 %\n"
     ]
    }
   ],
   "source": [
    "# trainとvalidの割合\n",
    "print(\"train: \", round(len(train_files)/ (len(train_files) + len(val_files)) * 100, 1), '%')\n",
    "print(\"valid: \", round(len(val_files)/ (len(train_files) + len(val_files)) * 100, 1), '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce75bd9-ed84-4500-a77c-9015f9c78f2c",
   "metadata": {
    "papermill": {
     "duration": 0.305658,
     "end_time": "2021-11-27T11:48:26.753060",
     "exception": false,
     "start_time": "2021-11-27T11:48:26.447402",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# ⚙️ Configuration\n",
    "\n",
    "datasetのconfig fileが必要\n",
    "1. datasetのroot　dierectory pathと`train / val / test`の画像ファイルのディレクトリ(もしくは、画像のpathが記載された*.txtファイル)を記載する\n",
    "2. クラスの数を`nc`として記載する\n",
    "3. クラスのリストを `names` : `['cots']`という形で記載する \n",
    "\n",
    "この辺りは実際にyamlの中身を見た方が早い"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "affbff01-62f7-48e0-9ed1-78dec73723e7",
   "metadata": {
    "papermill": {
     "duration": 0.353674,
     "end_time": "2021-11-27T11:48:27.412687",
     "exception": false,
     "start_time": "2021-11-27T11:48:27.059013",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "yaml:\n",
      "names:\n",
      "- cots\n",
      "nc: 1\n",
      "path: ../data/\n",
      "train: ../data/train.txt\n",
      "val: ../data/val.txt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "cwd = '../data/'\n",
    "\n",
    "with open(os.path.join( cwd , 'train.txt'), 'w') as f:\n",
    "    for path in train_df.image_path.tolist():\n",
    "        f.write(path+'\\n')\n",
    "        \n",
    "with open(os.path.join(cwd , 'val.txt'), 'w') as f:\n",
    "    for path in valid_df.image_path.tolist():\n",
    "        f.write(path+'\\n')\n",
    "\n",
    "# backgroundのクラスを追加してみる\n",
    "data = dict(\n",
    "    path  = '../data/',\n",
    "    train =  os.path.join( cwd , 'train.txt') ,\n",
    "    val   =  os.path.join( cwd , 'val.txt' ),\n",
    "    # nc    = 2,\n",
    "    nc = 1,\n",
    "    # names = ['cots', 'bg'],\n",
    "    names = ['cots']\n",
    "    )\n",
    "\n",
    "with open(os.path.join( cwd , 'tgbr.yaml'), 'w') as outfile:\n",
    "    yaml.dump(data, outfile, default_flow_style=False)\n",
    "\n",
    "f = open(os.path.join( cwd , 'tgbr.yaml'), 'r')\n",
    "print('\\nyaml:')\n",
    "print(f.read())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
